

# 机器学习

## sklearn的简介

```py
sklearn（sklearn为scikit-learn包名）是一个Python第三方提供的非常强力的机器学习库，它包含了从数据预处理到训练模型的各个方面。在实战使用Scikit-Learn中可以极大的节省我们编写代码的时间以及减少我们的代码量，使我们有更多的精力去分析数据分布，调整模型和修改超参
```

## scikit-learn基本概括

​	sklearn中的大部分函数可以归为**估计器**(Estimator)和**转化器**(Transformer)两类

### 估计器

​	**估计器(Estimator)**：其实就是模型，它用于对数据的预测或回归。

基本上估计器都会有以下几个方法：

- **fit(x,y)**：传入数据以及标签即可训练模型，训练的时间和参数设置，数据集大小以及数据本身的特点有关
- **score(x,y)**：用于对模型的正确率进行评分(范围0-1)。
  - 但由于对在不同的问题下，评判模型优劣的的标准不限于简单的正确率，可能还包括召回率或者是查准率等其他的指标，特别是对于类别失衡的样本，准确率并不能很好的评估模型的优劣，因此在对模型进行评估时，不要轻易的被score的得分蒙蔽。
- **predcit(x)**：用于对数据的预测，它接受输入，并输出预测标签，输出的格式为Numpy数组。我们通常使用这个方法返回测试的结果，再将这个结果用于评估模型

### 转化器

**转化器(Transformer)**：用于对数据的处理，例如标准化、降维以及特征选择等等。

同与估计器的使用方法类似：

- **fit(x,y)**：该方法接受输入和标签，计算出数据变换的方式。
- **transform(x)**：根据已经计算出的变换方式，返回对输入数据x变换后的结果（不改变x）
- **fit_transform(x,y)**：该方法在计算出数据变换方式之后对输入x就地转换

### 模型选择

对于一个分类任务，我们可以按照以下的图来选择一个比较合适的解决方法或者模型，但模型的选择并不是绝对的，事实上很多情况下你会去试验很多的模型，才能比较出适合该问题的模型。

![12](图\12.jpg)

### 数据划分

​	我们可以使用交叉验证或其他划分数据集的方法对数据集多次划分，以得出模型平均的性能而不是偶然结果。

​	sklearn有很多划分数据集的方法，它们都在model_selection里面，常用的有：

- K折交叉验证
- 留一法
- 随机划分法



### 常用模块

sklearn中常用的模块有**分类**、**回归**、**聚类**、**降维**、**模型选择**、**预处理**

#### 分类

**分类**：识别某个对象属于哪个类别；

常用的算法有：

- **SVM（支持向量机）**
- **nearest neighbors（最近邻-KNN）**
- **random forest（随机森林）**

常见的应用有：垃圾邮件识别、图像识别。

#### 回归

**回归**：预测与对象相关联的连续值属性；

常见的算法有：

- **SVR（支持向量机）**
- **ridge regression（岭回归）**
- **Lasso**

常见的应用有：药物反应，预测股价。

#### 聚类

**聚类**：将相似对象自动分组；

常见的算法有：

- **k-Means**
-  **spectral clustering**
- **mean-shift**

常见的应用有：客户细分，分组实验结果。

#### 降维

**降维**：减少要考虑的随机变量的数量

常见的算法有：

- **PCA（主成分分析）**
- **feature selection（特征选择）**
- **non-negative matrix factorization（非负矩阵分解）**

常见的应用有：可视化，提高效率。

#### 模型选择

**模型选择**：比较，验证，选择参数和模型

常用的模块有：

- **grid search（网格搜索）**
- **cross validation（交叉验证）**
-  **metrics（度量）**

它的目标是通过参数调整提高精度。

#### 预处理

**预处理**：特征提取和归一化

常用的模块有：

- **preprocessing**
- **feature extraction**

常见的应用有：把输入数据（如文本）转换为机器学习算法可用的数据。

## 模型选择

### 回归模型

#### 线性回归

在一元线性回归中，一个维度是响应变量，另一个维度是解释变量，总共两维。因此，其超平面只有一维，就是一条线。

最基本的线性回归法，它接收如下的几个参数：

```py
from sklearn.linear_model import LinearRegression
```

sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)

| 参数          | 解释                                       |
| ------------- | ------------------------------------------ |
| fit_intercept | 是否考察截距项b，默认为True                |
| normalize     | 是否先对数据进行Z-score标准化，默认为False |
| copy_X        | 默认为True则复制X，否则直接在原X上覆写     |
| n_jobs        | 使用的处理器核数，默认None表示单核         |

#### K近邻回归

k近邻算法的**核心思想**是未标记样本的类别，由距离其最近的k个邻居投票决定。

优点：该算法具有准确性高，对异常值和噪声有较高的容忍度等。缺点：是计算量比较大，内存消耗量也大。

```py
from sklearn.neighbor import KNeighborsRegressor
```

​	sklearn.neighbor.KNeighborsRegressor(n_neighbors=5, algorithm='auto')

| 参数        | 解释                                                         |
| ----------- | ------------------------------------------------------------ |
| n_neighbors | 最近邻单元的个数K                                            |
| weights     | 是否考虑邻居的权重，默认值"uniform"视每个邻居的权重相等，"distance"则给较近的单元更大的权重（取距离的倒数），也可以指定一个可调用的函数。 |
| algorithm   | 计算最近邻的算法，默认"auto"自动挑选模型认为最合适的，可选"ball_tree"、"kd_tree"、"brute" |
| leaf_size   | 叶节点数量，默认值30，只有在algorithm选择球树或者KD树时有效  |
| p           | 闵式距离的度量，p=1时为曼哈顿距离，p=2时为欧式距离（默认）   |
|             | 是最需要关注的超参数，其次weights和p也可以适当调整           |

#### 决策树(回归树)

```py
from sklearn.tree import DecisionTreeRegressor
```

`sklearn.tree.DecisionTreeRegressor`

| 参数              | 解释                                                         |
| ----------------- | ------------------------------------------------------------ |
| criterion         | 衡量分裂质量的函数，可以选择"mse"（均方误差）或"mae"（平均绝对误差） |
| splitter          | 分裂点选择策略，可以选择"best"或"random"                     |
| max_depth         | 树的最大深度                                                 |
| min_samples_split | 分裂所需的最小样本数                                         |
| min_samples_leaf  | 叶节点所需的最小样本数                                       |
| max_features      | 寻找最佳分裂点时需要考虑的特征数                             |
| random_state      | 随机种子                                                     |
|                   |                                                              |



#### 随机森林回归

**随机森林**

```py
from sklearn.ensemble import RandomForestRegressor
```

`sklearn.ensemble.RandomForestRegressor`(n_estimators=10, criterion='gini', max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)

| 参数(共同)               | 解释                                                         |
| ------------------------ | ------------------------------------------------------------ |
| n_estimators             | 树的数量，默认为10                                           |
| criterion                | 分枝的标准，默认"mse"为均方差，可选"mae"（绝对平均误差）     |
| max_depth                | 限制树的最大深度，默认值为None，表示一直分枝直到所有叶节点都是纯的，或者所有叶节点的样本数小于min_samples_split |
| min_samples_split        | 分割一个节点所需的最小样本数，默认为2                        |
| min_samples_leaf         | 叶节点上所需的最小样本数，叶节点样本数少于这个值时会被剪枝。默认为1 |
| min_weight_fraction_leaf | 叶节点样本权重和所需的最小值，默认为0即视样本具有相同的权重  |
| max_features             | 寻找最佳分割时要考虑的特征数                                 |
| bootstrap                | 是否在构建树时使用 bootstrap 样本                            |

| 参数(区别) | 解释                                     |
| ---------- | ---------------------------------------- |
| criterion  | 衡量拆分质量的函数，可以是“mse”或“mae”； |
| n_targets_ | 回归问题的目标数                         |
|            |                                          |



#### AdaBoost回归

```py
from sklearn.ensemble import AdaBoostRegressor
```

|      参数      |                             解释                             |
| :------------: | :----------------------------------------------------------: |
| base_estimator | 弱回归学习器，可指定为任意回归模型对象，默认为None，即DecisionTreeRegressor（max_depth=3） |
|  n_estimators  |         最大迭代次数，即弱学习器的最大个数，默认为50         |
| learning_rate  |     每个弱学习器的权重缩减系数，介于0.和1.之间，默认为1      |
|      loss      | 每次迭代后更新权重时采用的损失函数，默认为"linear"可"square"、"exponential"，通常使用默认值 |
|  random_state  |                          随机数种子                          |



### 分类模型

#### 逻辑回归

```py
from sklearn.metrics import LogisticRegressionCV
```

`LogisticRegressionCV`是一个用于拟合带有交叉验证的逻辑回归模型的类，它会自动进行交叉验证，并根据交叉验证的结果选择最佳的超参数，这样可以更好地避免过拟合和欠拟合问题

sklearn.metrics.LogisticRegressionCV

| 参数          | 解释                                                         |
| ------------- | ------------------------------------------------------------ |
| Cs            | 浮点列表或者整型，如果为整型；C越小，正则项对系数的惩罚性越强 |
| fit_intercept | 布尔型，是否考虑截距项。默认为True                           |
| cv            | 交叉验证折数，默认None代表3                                  |
| penalty       | 采用何种正则化，默认"l2"，可选"l1"，但注意使用"newton-cg"、"sag"和"lbfgs"这三种优化算法时仅支持"l2" |
| scoring       | 评分函数，默认使用"accuracy"准确度                           |
| solver        | 优化算法，可选"newton-cg"、"lbfgs"（默认）、"liblinear"、"sag"、"saga" |
|               | 对于小数据集可选"liblinear"，巨型数据集选择随机梯度下降"sag"或"saga"更快；此外，进行多分类任务尽量不选择"liblinear"，因为其只能采用一对多的分类方式 |
| max_iter      | 优化算法的最大迭代次数                                       |
| class_weight  | 类别权重，默认视所有类别具有相同的权重，可选"balanced"自动按照类别频率分配权重，也可指定一个字典 |
| random_state  | 随机数种子                                                   |
| multi_class   | 多分类时的分类策略，可选"ovr"（默认）、"multinomial"、"auto"。"ovr"即一对多，迭代快、准确性不如多对多；"multinomial"为多对多，迭代慢、准确度高。当优化算法使用"liblinear"时无法使用"multinomial" |



#### 决策树(分类树)

```py
from sklearn.tree import DecisionTreeClassifier
```

sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=None)

| 参数                     | 解释                                                         |
| ------------------------ | ------------------------------------------------------------ |
| criterion                | 分枝的标准，默认"gini"为基尼不纯度，可选"entropy"信息增益    |
| max_depth                | 限制树的最大深度，默认值为None                               |
| splitter                 | 分枝的策略，默认"best"在所有划分点中找出最优的划分点，适合样本量不大的情况 |
| min_samples_split        | 分裂所需的最小样本数，默认为2，当样本量**非常大**时可以增加这个值 |
| min_samples_leaf         | 叶节点所需的最小样本数，默认为2，当样本量**非常大**时可以增加这个 |
| min_weight_fraction_leaf | 叶节点样本权重和所需的最小值，默认为0即视样本具有相同的权重  |
| random_state             | 随机数种子                                                   |



#### 随机森林分类

```py
from sklearn.ensemble import RandomForestClassifier
```

`sklearn.ensemble.RandomForestClassifier`

| 参数(共同)        | 解释                                  |
| ----------------- | ------------------------------------- |
| n_estimators      | 决策树的数量                          |
| criterion         | 特征选择准则，可以是"gini"或"entropy" |
| max_depth         | 决策树的最大深度                      |
| min_samples_split | 拆分内部节点所需的最小样本数          |
| min_samples_leaf  | 在叶节点处所需的最小样本数            |
| max_features      | 寻找最佳分割时要考虑的特征数          |
| bootstrap         | 是否在构建树时使用 bootstrap 样本     |

| 参数(区别)   | 解释                               |
| ------------ | ---------------------------------- |
| class_weight | 用于处理不平衡的类别数据           |
| n_classes_   | 分类问题的类别数                   |
| warm_start   | 如果设置为True，则可以重复拟合模型 |





#### 支持向量机

```py
from sklearn.svm import SVC
```

| 参数         | 解释                                                         |
| ------------ | ------------------------------------------------------------ |
| C            | 惩罚系数C，默认值为1.0                                       |
| kernel       | 核函数，默认使用"rbf"径向基函数，可选"linear"、"poly"、"sigmoid"、"precomputed"或者一个可调用的函数 |
| degree       | 多项式核函数的维度d，仅在核函数选择"poly"时有效。默认值为3   |
| gamma        | "rbf"、"poly"、"sigmoid"的系数gamma，默认为"auto"，取特征数量的倒数，如果使用"scale"，则取特征数量乘以变量二阶矩再取倒数 |
| coef         | 核函数中的独立项，仅在核函数选择"poly"、"sigmoid"时有效。默认值为0.0 |
| shrinking    | 是否使用shrinking heuristic方法，默认为True                  |
| probability  | 是否使用概率估计，默认为False                                |
| tol          | 停止训练的误差精度，默认值为1e-3                             |
| max_iter     | 最大迭代次数，默认为-1即无限制                               |
| random_state | 随机数种子                                                   |

最重要的两个调参对象是gamma和C。gamma越大，支持向量越少，gamma越小，支持向量越多。C可理解为逻辑回归中正则项系数lambda的倒数，C过大容易过拟合，C过小容易欠拟合。通常采用网格搜索法进行调参

#### KNN

```py
from sklearn.neighbor import KNeighborsClassifier
```

​	sklearn.neighbor.KNeighborsClassifier(n_neighbors=5, algorithm='auto')

| 参数        | 解释                                                         |
| ----------- | ------------------------------------------------------------ |
| n_neighbors | 最近邻单元的个数K，默认为5                                   |
| weights     | 是否考虑邻居的权重，默认值"uniform"视每个邻居的权重相等，"distance"则给较近的单元更大的权重（取距离的倒数），也可以指定一个可调用的函数 |
| algorithm   | 计算最近邻的算法，默认"auto"自动挑选模型认为最合适的，可选"ball_tree"、"kd_tree"、"brute" |
| leaf_size   | 叶节点数量，默认值30，只有在algorithm选择球树或者KD树时有效  |
| p           | 闵式距离的度量，p=1时为曼哈顿距离，p=2时为欧式距离（默认）   |

  n_neighbors是最需要关注的超参数，其次weights和p也可以适当调整。



##### **朴素贝叶斯算法**

**API**

```py
sklearn.naive_bayes.MultinomialNB(alpha=1.0)
```

**20类新闻分类案例**

**1 步骤分析**
**1）获取数据**
**2）划分数据集**
**3）特征工程：文本特征抽取**
**4）朴素贝叶斯预估器流程**
**5）模型评估**

```py
from sklearn.model_selection import train_test_split  # 划分数据集
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer  # 文本特征提取
from sklearn.naive_bayes import MultinomialNB  # 朴素贝叶斯


def nb_news():
    '''
    用朴素贝叶斯算法对新闻进行分类
    :return:
    '''
    # 1.获取数据
    news = fetch_20newsgroups(subset='all')

    # 2.划分数据集
    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target)

    # 3.特征工程: 文本特征提取
    transfer = TfidfVectorizer()
    # 模型会先对训练数据进行拟合（fit）操作，计算出特征提取的参数，然后将其应用于训练数据，返回转换后的数据
    x_train = transfer.fit_transform(x_train)
    # 模型不会进行拟合操作，而是直接使用之前计算得到的参数，将测试数据转换为与训练数据相同的特征矩阵
    x_test = transfer.transform(x_test)

    # 4.朴素贝叶斯算法预估器流程
    estimator = MultinomialNB()
    estimator.fit(x_train, y_train)

    # 5.模型评估
    # 方法1
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接必读真实值和预测值：\n", y_test == y_predict)  # 直接比对

    # 方法2：
    score = estimator.score(x_test, y_test)
    print('准确率：', score)

    return None


if __name__ == "__main__":
    nb_news()
```



**3.4.7 朴素贝叶斯算法总结**

**优点：**

- **朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率**
- **对缺失数据不太敏感，算法也比较简单，常用于文本分类**
- **分类准确度高，速度快**
  **缺点：**
- **由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好**

### 聚类模型

#### 聚类

##### K-Means

```py
from sklearn.cluster import KMeans
```

| 参数                 | 解释                                                         |
| -------------------- | ------------------------------------------------------------ |
| n_clusters           | 要分成的类别数，默认值为8                                    |
| init                 | 初始化聚类中心的方法，默认为"k-means++"，它将智能选择初始聚类中心；"random"将随机选择初始聚类中心；也可传入一组数组指定为初始聚类中心。 |
| n_init               | 用不同的初始化聚类中心运行算法的次数，默认10                 |
| max_iter             | 最大迭代次数，默认300                                        |
| tol                  | 容差，默认1e-4                                               |
| precompute_distances | 是否预先计算距离（更快但消耗更多内存）。选"auto"会在n_samples * n_clusters > 12时不预先计算距离 |
| verbose              | 是否冗余输出，默认0                                          |
| random_state         | 随机数种子                                                   |
| copy_x               | 是否复制训练集，默认为True，如果为False则会直接在原数据上修改 |
| n_jobs               | 使用的核心数。默认None为单核                                 |
| algorithm            | 可选"auto"（默认）、"full"或"elkan"，"auto"自动选择"elkan"处理稠密数据，"full"处理稀疏数据 |
|                      |                                                              |
| cluster_centers_     | 返回聚类中心                                                 |
| labels_              | 返回聚类后每个点的标签                                       |
| inertia_             | 返回样本到最近聚类中心距离的平方和                           |
| n_iter_              | 返回迭代次数                                                 |

##### DBSCAN



### 降维

#### PCA

| 参数           | 解释                                                         |
| -------------- | ------------------------------------------------------------ |
| n_components   | 降维之后保留的维数，如果不指定则该值取样本数和特征数间的较小值。可指定为一个整数，即所需保留的维数。如果设为"mle"且svd_solver == "full"，会采用MLE算法自动选择一个合适的维度 |
| copy           | 是否创建数据副本而不覆盖原数据，默认为True                   |
| whiten         | 是否白化，即去除降维后的特征之间的相关性并且方差相同。默认为False |
| svd_solver     | SVD采用的算法，默认"auto"会根据输入数据自动挑选最优解，具体方法有"full"、"arpack"、"randomized" |
| tol            | svd_solver == "arpack"时，奇异值的误差容忍度，默认0          |
| iterated_power | svd_solver == "randomized"时的迭代次数，默认"auto"           |
| random_state   | 随机数种子                                                   |

PCA属性：

| 参数                      | 解释                         |
| ------------------------- | ---------------------------- |
| components_               | 主成分的轴的方向             |
| explained_variance_       | 降维后各成分的方差           |
| explained_variance_ratio_ | 降维后各成分的方差所占的比值 |
| singular_values_          | 降维后主成分的奇异值         |
| mean_                     | 降维后主成分的经验均值       |
| n_components_             | 降维之后保留的维数           |
| noise_variance_           | 噪声协方差                   |



## 预处理

### 归一化

​	归一化是将数据缩放到[0,1]的区间内，通常使用Min-Max Scaler（最小最大规范化）来实现。

大多数机器学习算法中，会选择`StandardScaler`来进行特征缩放，因为`MinMaxScaler`对异常值非常敏感。在**PCA**，**聚类**，**逻辑回归**，支持**向量机**，神经网络这些算法中`StandardScaler`往往是最好的选择

**1 定义**
**通过对原始的数据进行变换把数据映射到（默认为[0,1]之间）**

，归一化后数据的范围在[0,1]之间

**2 API**

```py
from sklearn.preprocessing import MinMaxScaler

transform = MinMaxScaler()
transform.fit_transform(X)

#feature_range：指定输出范围，默认为 (0, 1)；
#copy：是否在归一化前复制数据，默认为 True；
#clip：是否在归一化后将数据截取到给定范围，默认为 False；
#n_features_in_ 和 n_samples_seen_：用于记录已经处理的样本数和特征数的属性。
```

**X：numpy array格式的数据[n_samples,n_features]，返回值：转换后的形式相同的array**

```py
import pandas as pd
from sklearn.preprocessing import MinMaxScaler


def minmax_demo():
    """
    归一化
    :return:
    """
    # 1、获取数据
    data = pd.read_csv("datingTestSet2.txt", sep='\t')
    data = data.iloc[:, :3]
    print("data:\n", data)

    # 2、实例化一个转换器类
    transform = MinMaxScaler()
    #transform = MinMaxScaler(feature_range=[2,3])

    # 3、调用fit_transform
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)

    return None


if __name__ == "__main__":
    minmax_demo()
```

**3 归一化总结**
**注意最大值最小值是变化的，另外，最大值与最小值非常容易受到异常值影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景**



### 标准化

**1 定义**
通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内

**标准化后的数据范围则没有限制，一般会在[-3,3]之间**

对于某些需要计算距离的算法，如**K近邻**和**支持向量机**等，标准化是**必不可少**的预处理步骤

```py
from sklearn.perprocessing import StandradScaler

transform = StandradScaler()
transform.fit_transform(X)
# 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
```

| 参数          | 解释                                               |
| ------------- | -------------------------------------------------- |
| fit           | 计算数据的均值和标准差                             |
| transform     | 用于对数据进行标准化处理                           |
| fit_transform | 可以在一次调用中完成数据的均值计算和标准化处理     |
| with_mean     | 布尔型参数，表示是否对数据进行均值移除，默认为True |
| with_std      | 布尔型参数，表示是否对数据进行缩放，默认为True     |



```py
from sklearn.preprocessing import MinMaxScaler, StandardScaler


def stand_demo():
    """
    标准化
    :return:
    """
    # 1、获取数据
    data = pd.read_csv("datingTestSet2.txt", sep='\t')
    data = data.iloc[:, :3]
    print("data:\n", data)

    # 2、实例化一个转换器类
    transform = StandardScaler()
    #transform = StandardScaler(feature_range=[2,3])

    # 3、调用fit_transform
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)

    return None


if __name__ == "__main__":
    stand_demo()
```

**2 标准化总结**
**在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景**

**3 归一化和标准化的区别**

- **对于归一化来说：如果出现异常点，影响了最大值和最小值，name结果显然会发生改变**
- **对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响不大，从而方差改变较小**



### 离散化

```py
from sklearn.preprocessing import KBinsDiscretizer
```

| 参数                         | 解释                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| n_bins                       | 离散化后的总箱数，默认为5                                    |
| encode                       | 指定输出数组中每个特征的编码方式，可选参数包括"onehot"、"onehot-dense"和"ordinal"，默认为"onehot" |
| strategy                     | 指定每个特征的离散化策略，可选参数包括"uniform"、"quantile"和"kmeans"，默认为"quantile" |
| strategy参数的具体含义如下： |                                                              |
| uniform                      | 将数据分为n_bins个等宽的区间                                 |
| quantile                     | 根据数据的分位数将数据分为n_bins个区间                       |
| kmeans                       | 使用k-means算法将数据分为n_bins个区间                        |
|                              |                                                              |

离散化可以适用于很多模型，特别是那些只接受离散输入的模型。例如，**决策树**、**朴素贝叶斯**和**规则学习**算法等都可以接受离散特征输入

# --------------------------------------------------

### **机器学习需导入的库**

**![1](图\1.png)**

```py
# 1.导入所需的库
from sklearn.datasets import load_iris  # 导入数据，鸢尾花数据
from sklearn.model_selection import train_test_split  # 用于划分数据
from sklearn.preprocessing import StandardScaler  # 标准化
from sklearn.neighbors import KNeighborsClassifier  # KNN
from sklearn.tree import DecisionTreeClassifier  # 决策树
from sklearn.naive_bayes import MultinomialNB  # 朴素贝叶斯
from sklearn.ensemble import RandomForestClassifier  # 随机森林
# 线性回归和逻辑回归、岭回归
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge
from sklearn.cluster import KMeans  # K-means
from sklearn.svm import SVC  # SVM
```

```py
# 划分数据集
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

```

```py
# 划分完数据后，进行数据的归一化或者标准
#特征预处理
# ↓↓↓标准化↓↓↓
# 通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内
from sklearn.preprocessing import StandardScaler

transform = StandardScaler()
data_new = transform.fit_transform(data)

# ↓↓↓归一化↓↓↓
# 通过对原始的数据进行变换把数据映射到（默认为[0,1]之间）
from sklearn.preprocessing import MinMaxScaler()

transform = MinMaxScaler()
data_new = transform.fit_transform(data)
```

```py
# 导入模型
from sklearn.neighbors import KNeighborsClassifier  # KNN
from sklearn.tree import DecisionTreeClassifier  # 决策树
from sklearn.naive_bayes import MultinomialNB  # 朴素贝叶斯
from sklearn.ensemble import RandomForestClassifier  # 随机森林
# 线性回归和逻辑回归、岭回归
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge
from sklearn.cluster import KMeans  # K-means
from sklearn.svm import SVC  # SVM



```



#### **机器学习算法分类**

##### **监督学习：预测**

- **定义：输入数据是由输入特征和目标值所组成，函数的输出可以是一个连续的值，称为回归；也可以是一个离散的值，称为分类**
- **分类：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归**
- **回归：线性回归、岭回归**

##### **无监督学习**

- **定义：输入数据是由输入特征值组成**
- **聚类：k-means**

#### **scikit-learn数据集API**

**1、sklearn.datasets.load_*()：获取小规模数据集，数据包含在datasets里_**

**2、sklearn.datasets.fetch_*(data_home=None)：获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录，默认是~/scikit_learn_data/**

#### **数据集的划分**

- **训练数据：用于训练，构建模型**
- **测试数据：在模型检验时使用，用于评估模型是否有效**

**划分比例：**

- **训练集：70%、80%**
- **测试集：30%、20%**

**数据集划分api：sklearn.model_selection.train_test_split(arrays, *options)**

- **x 数据集的特征值**
- **y 数据集的标签值**
- **test_size测试集的大小，一般为float**
- **random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。**
- **return训练集特征值，测试集特征值，训练集目标值，测试集目标值**

#### **特征工程：**

##### **特征提取API：**

```py
sklearn.feature_extraction
```

##### **字典特征提取**

**作用：对字典数据进行特征值化**

**sklearn.feature_extraction.DictVectorizer(sparse=True, …)**

**DictVectorizer.fit_transform(X)， X：字典或者包含字典的迭代器返回值，返回sparse矩阵**
**DictVectorizer.inverse_transform(X)， X：array数组或者sparse矩阵 返回值：转换之前数据格式**
**DictVectorizer.get_feature_names()：返回类别名称**



**对数据进行特征提取：将类别转换为one-hot编码，节省内存，提高下载效率**

```py
from sklearn.feature_extraction import DictVectorizer

def dict_demo():
    """
    字典特征抽取
    :return:
    """
    data = [{'city':'北京', 'temperature':100},
            {'city':'上海', 'temperature':60},
            {'city':'深圳', 'temperature':30}]
    # 1、实例化一个转换器类
    #transfer = DictVectorizer() # 返回sparse矩阵
    transfer = DictVectorizer(sparse=False)
    # 2、调用fit_transform()
    data_new = transfer.fit_transform(data)
    print("data_new：\n", data_new)   # 转化后的
    print("特征名字：\n", transfer.get_feature_names())

    return None


if __name__ == "__main__":
    dict_demo()
```

##### **文本特征提取**

**单词作为特征      作用：对文本数据进行特征值化**

**sklearn.feature_extraction.text.CountVectorizer(stop_words=[])：返回词频矩阵**

- **CountVectorizer.fit_transform(X)，X：文本或者包含文本字符串的可迭代对象，返回值：返回sparse矩阵**
- **CountVectorizer.inverse_transform(X)，X：array数组或者sparse矩阵，返回值：转换之前数据格**
- **CountVectorizer.get_feature_names()：返回值：单词列表**

**英文文本分词**

```py
from sklearn.feature_extraction.text import CountVectorizer


def count_demo():
    """
    文本特征抽取：CountVectorizer
    :return:
    """
    data = ['life is short,i like like python',
            'life is too long,i dislike python']
    # 1、实例化一个转换器类
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data_new = transfer.fit_transform(data)
    print("data_new：\n", data_new.toarray())  # toarray转换为二维数组
    print("特征名字：\n", transfer.get_feature_names())

    return None


if __name__ == "__main__":
    count_demo()
```

**运行结果：**

```py
data_new：
 [[0 1 2 0 1 1]
 [1 1 0 1 1 0]]
特征名字：
 ['dislike', 'life', 'like', 'long', 'python', 'short']

```



**中文文本分词**

**注意：不支持单个中文词！**
**这个方法是计算特征词出现的个数的**

```py
from sklearn.feature_extraction.text import CountVectorizer


def count_demo():
    """
    文本特征抽取：CountVectorizer
    :return:
    """
    data = ['我 爱 北京 天安门',
            '天安门 上 太阳 升']
    # 1、实例化一个转换器类
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data_new = transfer.fit_transform(data)
    print("data_new：\n", data_new.toarray())  # toarray转换为二维数组
    print("特征名字：\n", transfer.get_feature_names())

    return None


if __name__ == "__main__":
    count_demo()

```

```py
data_new：
 [[1 1 0]
 [0 1 1]]
特征名字：
 ['北京', '天安门', '太阳']
```

**例2**

```py
from sklearn.feature_extraction.text import CountVectorizer
import jieba


def count_chinese_demo2():
    """
    中文文本特征抽取，自动分词
    :return:
    """
    data = ['一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。',
            '我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。',
            '如果只用一种方式了解某件事物，他就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。']
    data_new = []
    for sent in data:
        data_new.append(cut_word(sent))
    print(data_new)

    # 1、实例化一个转换器类
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data_final = transfer.fit_transform(data_new)
    print("data_final:\n", data_final.toarray())
    print("特征名字：\n", transfer.get_feature_names())

    return None


def cut_word(text):
    """
    进行中文分词：“我爱北京天安门” -> "我 爱  北京 天安门"
    :param text:
    :return:
    """

    return ' '.join(jieba.cut(text))


if __name__ == "__main__":
    count_chinese_demo2()
    #print(cut_word('我爱北京天安门'))

```

**运行结果：**

```py
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某件事 物 ， 他 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
data_final:
 [[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1
  0]
 [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0
  1]
 [1 1 0 0 4 2 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0
  0]]
特征名字：
 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某件事', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']

```

**关键词：在某一个类别的文章中，出现的次数很多，但是在其他类别的文章当中出现很少**



**Tf-idf文本特征提取**

**Tf-idf的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分来**

**Tf-idf作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度**

**这种方法是计算特征词的重要程度的**

**TF-IDF:衡量重要程度**

**TF：词频**

**IDF：逆向文档频率，可以由总文件数目 / 包含该词语之文件的数目，再将得到的商取以10为底的对数得到**

```py
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import jieba


def cut_word(text):
    """
    进行中文分词：“我爱北京天安门” -> "我 爱  北京 天安门"
    :param text:
    :return:
    """

    return ' '.join(jieba.cut(text))


def tfidf_demo():
    """
    用TF-IDF的方法进行文本特征抽取
    :return:
    """
    data = ['一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。',
            '我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。',
            '如果只用一种方式了解某件事物，他就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。']
    data_new = []
    for sent in data:
        data_new.append(cut_word(sent))
    print(data_new)

    # 1、实例化一个转换器类
    transfer = TfidfVectorizer()
    # 2、调用fit_transform
    data_final = transfer.fit_transform(data_new)
    print("data_final:\n", data_final.toarray())
    print("特征名字：\n", transfer.get_feature_names())
    return None

if __name__ == "__main__":
    tfidf_demo()
    #print(cut_word('我爱北京天安门'))

```

**运行结果：**

```py
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某件事 物 ， 他 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
data_final:
 [[0.30847454 0.         0.20280347 0.         0.         0.
  0.40560694 0.         0.         0.         0.         0.
  0.20280347 0.         0.20280347 0.         0.         0.
  0.         0.20280347 0.20280347 0.         0.40560694 0.
  0.20280347 0.         0.40560694 0.20280347 0.         0.
  0.         0.20280347 0.20280347 0.         0.         0.20280347
  0.        ]
 [0.         0.         0.         0.2410822  0.         0.
  0.         0.2410822  0.2410822  0.2410822  0.         0.
  0.         0.         0.         0.         0.         0.2410822
  0.55004769 0.         0.         0.         0.         0.2410822
  0.         0.         0.         0.         0.48216441 0.
  0.         0.         0.         0.         0.2410822  0.
  0.2410822 ]
 [0.12826533 0.16865349 0.         0.         0.67461397 0.33730698
  0.         0.         0.         0.         0.16865349 0.16865349
  0.         0.16865349 0.         0.16865349 0.16865349 0.
  0.12826533 0.         0.         0.16865349 0.         0.
  0.         0.16865349 0.         0.         0.         0.33730698
  0.16865349 0.         0.         0.16865349 0.         0.
  0.        ]]
特征名字：
 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某件事', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']

```

#### **特征预处理**

##### **归一化**

**1 定义**
**通过对原始的数据进行变换把数据映射到（默认为[0,1]之间）**

**2 API**

```py
from sklearn.preprocessing import MinMaxScaler

transform = MinMaxScaler()
transform.fit_transform(X)
```

**X：numpy array格式的数据[n_samples,n_features]，返回值：转换后的形式相同的array**

```py
import pandas as pd
from sklearn.preprocessing import MinMaxScaler


def minmax_demo():
    """
    归一化
    :return:
    """
    # 1、获取数据
    data = pd.read_csv("datingTestSet2.txt", sep='\t')
    data = data.iloc[:, :3]
    print("data:\n", data)

    # 2、实例化一个转换器类
    transform = MinMaxScaler()
    #transform = MinMaxScaler(feature_range=[2,3])

    # 3、调用fit_transform
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)

    return None


if __name__ == "__main__":
    minmax_demo()
```

**3 归一化总结**
**注意最大值最小值是变化的，另外，最大值与最小值非常容易受到异常值影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景**



##### **标准化**

**1 定义**
**通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内**

```py
from sklearn.perprocessing import StandradScaler

transform = StandradScaler()
transform.fit_transform(X)
# 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
```

```py
from sklearn.preprocessing import MinMaxScaler, StandardScaler


def stand_demo():
    """
    标准化
    :return:
    """
    # 1、获取数据
    data = pd.read_csv("datingTestSet2.txt", sep='\t')
    data = data.iloc[:, :3]
    print("data:\n", data)

    # 2、实例化一个转换器类
    transform = StandardScaler()
    #transform = StandardScaler(feature_range=[2,3])

    # 3、调用fit_transform
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)

    return None


if __name__ == "__main__":
    stand_demo()
```

**2 标准化总结**
**在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景**

**3 归一化和标准化的区别**

- **对于归一化来说：如果出现异常点，影响了最大值和最小值，name结果显然会发生改变**
- **对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响不大，从而方差改变较小**



#### **特征降维**

**1 定义**
**数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征**

**2 低方差特征过滤**

**删除低方差的一些特征**

- **特征方差小：某个特征大多样本的值比较相近**
- **特征方差大：某个特征很多样本的值都有差别**

**3API**

```py
sklearn.feature_selection.VArianceThreshold(threshold=0.0)
```

**4数据计算**

```py
from sklearn.feature_selection import VarianceThreshold


def variance_demo():
    """
    低方差特征过滤
    :return:
    """
    # 1、获取数据
    data = pd.read_csv('factor_returns.csv')
    print('data:\n', data)
    data = data.iloc[:,1:-2]
    print('data:\n', data)

    # 2、实例化一个转换器类
    #transform = VarianceThreshold()
    transform = VarianceThreshold(threshold=10)

    # 3、调用fit_transform
    data_new = transform.fit_transform(data)
    print("data_new\n", data_new, data_new.shape)

    return None

if __name__ == "__main__":
    variance_demo()

```

**![3](图\3)**

#### **sklearn转换器和估计器**

##### **转换器**

**1.实例化一个转换器**

**2.调用fit_transform()**

```py
transfer = TfidfVectorizer()
```

**转换器调用有以下几种形式：**

- **fit_transform**
- **fit**
- **transform**

##### **估计器**

**在sklearn中，估计器是一个重要的角色，是一类实现了算法的API**

**1、用于分类的估计器：**

- **sklearn.neighbors k近邻算法**
- **sklearn.native_bayes 贝叶斯**
- **sklearn.linear_model.LogisticRegression 逻辑回归**
- **sklearn.tree 决策树与随机森林**

**2、用于回归的估计器**

- **sklearn.linear_model.LinearRegression 线性回归**
- **sklearn.linear_model.Ridge 岭回归**

**3、用于无监督学习的估计器**

- **sklearn.cluster.KMeans 聚类**

###### **估计器工作流程**

**![2](图\2.png)**

**估计器（estimator）:**

- **1、实例化一个estimator**
- **2、estimator.fit(x_train, y_train)计算**
  **-----调用完毕，模型生成**
- **3、模型评估：**
  - **1）直接比对真实值和预测值：y_predict = estimator.predict(x_test)**
  - **2）计算准确率：accuracy = estimator.score(x_test, y_test)**

#### **分类算法**

##### **K-近邻算法(KNN)**

**K-近邻算法API**

```py
sklearn.neighbor.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
```

- **n_neighbors：int型，k_neighbors查询默认使用的邻居数**
- **algorithm：{‘auto’，‘ball_tree’，‘kd_tree’}之一**

**流程：**
**1）获取数据**
**2）数据集划分**
**3）特征工程：标准化**
**4）KNN预估器流程**
**5)模型评估**

```py
from sklearn.datasets import load_iris  # 获取数据
from sklearn.model_selection import train_test_split  # 划分数据集
from sklearn.preprocessing import StandardScaler  # 标准化
from sklearn.neighbors import KNeighborsClassifier  # KNN算法分类


def knn_iris():
    """
    用KNN算法对鸢尾花进行分类
    :return:
    """
    # 1.获取数据
    iris = load_iris()

    # 2.划分数据集
    #  x_train和y_train是训练集，x_test和y_test是测试集。
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=6)
    #  x_train训练集的特征矩阵   x_test测试集的特征矩阵   y_train训练集的标签向量  y_test测试集的标签向量

    #  3.特征工程:标准化
    transfer = StandardScaler()
    #  处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
    x_train = transfer.fit_transform(x_train)  # 把训练数据矩阵标准化
    x_test = transfer.fit_transform(x_test)  # 把测试集矩阵标准化

    #  4.Knn算法预估器
    estimator = KNeighborsClassifier(n_neighbors=3)  # 实例化,表示选择最近的3个样本进行预测
    estimator.fit(x_train, y_train)  # 把训练集特征矩阵和标签向量训练

    # 5.模型评估
    # 方法1：直接比对知识值和预测值
    # y_predict = estimator.predict(x_test)
    # print('Y_predict:\n', y_predict)
    # print('直接必读真实值和预测值:', y_test == y_predict)

    #  方法2：计算准确率
    score = estimator.score(x_test, y_test)  # 测试集的特征值，测试机的目标值
    print("准确率：\n", score)
    return None


if __name__ == "__main__":
    knn_iris()
```

**运行结果**

```py
y_predict:
 [0 2 0 0 2 1 1 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1 2]
直接必读真实值和预测值：
 [ True  True  True  True  True  True False  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率：
 0.9210526315789473
```



**K-近邻总结**

**优点：简单，易于实现，无需训练**
**缺点：懒惰算法，对测试样本分类时的计算量大，内存开销大；必须指定K值，K值选择不当则分类精度不能保证**
**使用场景：小数据场景，几千~几万样本**





##### **决策树**

**决策树API**

```py
sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=None)
```

- **决策树分类器**
- **criterion：默认是“gini”系数，也可以选择信息增益的熵‘entropy’**
- **max_depth：树的深度大小**
- **random_state：随机数种子**

**决策树用于鸢尾花数据集**

```py
from sklearn.datasets import load_iris  # 获取数据集
from sklearn.model_selection import train_test_split  # 划分数据集
from sklearn.tree import DecisionTreeClassifier  # 决策树


def decision_iris():
    """
    决策树鸢尾花进行分类
    :return:
    """
    # 1.获取数据
    iris = load_iris()

    # 2.划分数据集
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)

    # 不用做特征工程：标准化
    # 3.决策树预估器
    estimator = DecisionTreeClassifier()
    estimator.fit(x_train, y_train)

    # 4模型评估
    #方法1
    y_predict = estimator.predict(x_test)
    print("y_predict：\n", y_predict)
    print("直接必读真实值和预测值:\n", y_test == y_predict)  # 直接比对

    #方法2：
    score = estimator.score(x_test,y_test)
    print("准确率：",score)
    return None


if __name__ == "__main__":
    decision_iris()
```

```py
# 可视化决策树
from sklearn.tree import export_graphviz  # 决策树可视化
export_graphviz(estimator, out_file='iris_tree.dot', feature_names=iris.feature_names)
```

**生成的可视化**

**![4](图\4.png)**

**决策树总结**

- **优点：简单的理解和解释，树木可视化**
- **缺点：决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合**

**改进：**

- **剪枝cart算法（决策树API当中已经实现，随机森林参数调优有相关介绍）**




##### **随机森林**

**1.随机森林原理过程**

**两个随机：**

- **训练集随机：BoostStrap，N个样本中随机有放回抽样**
- **特征值随机：从M个特征中随机抽取m个特征，M>>m**

**训练集：特征值、目标值**

**2.为什么采用BootStrap抽样**

**1、为什么要随机抽样训练集？**
**如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的**

**2、为什么要有放回地抽样？**
**如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树的投票表决**

**3.API随机森林分类器**

```
sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
```

- **n_estimators：integer，optional(default=10)森林里的树木数量，可以用网格搜索**
- **criteria：string，可选（default=‘gini’），分割特征的测量方法**
- **max_depth：integer或None，可选（默认无），树的最大深度5，8，15，25，30 可以用网格搜索**
- **max_teatures=‘auto’，每个决策树的最大特征数量**
  - **if ‘auto’ ，then max_features = sqrt(n_features)**
  - **if ‘sqrt’ ，then max_features = sqrt(n_features)**
  - **if ‘log2’ ，then max_features = log2(n_features)**
  - **if None，then max_features = n_features**
- **booststrap：boolean，optional（default=True）是否在构建树时使用放回抽样**
- **min_samples_split：节点划分最少样本数**
- **min_samples_leaf：叶子节点的最小样本数**
- **超参数：n_estimator，max_depth，min_samples_split，min_samples_leaf**

 **随机森林案例预测**

```py
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier


def unde():
    # 1.获取数据
    path = 'titanic.csv'
    titanic = pd.read_csv(path)

    # 筛选特征值和目标值
    x = titanic[["pclass", "age", "sex"]]
    y = titanic["survived"]

    # 2.数据处理
    # 1.处理缺失值
    x['age'].fillna(x['age'].mean(), inplace=True)

    # 2.转换成字典
    x = x.to_dict(orient="records")

    # 3.数据集划分
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22)

    # 4.字典特征抽取
    transfer = DictVectorizer()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)

    # # 4.决策树预估器
    # estimator = DecisionTreeClassifier(criterion="entropy")
    # estimator.fit(x_train, y_train)

    estimator = RandomForestClassifier(max_depth=5, n_estimators=120)

    # 参数准备
    param_dict = {"n_estimators": [120, 200, 300, 500, 800, 1200], "max_depth": [5, 8, 15, 25, 30]}
    estimator = GridSearchCV(estimator, param_grid=param_dict, cv=3)  # 10折，数据量不大，可以多折

    estimator.fit(x_train, y_train)

    # 5、模型评估
    # 方法1：直接比对真实值和预测值
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接必读真实值和预测值：\n", y_test == y_predict)  # 直接比对

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)  # 测试集的特征值，测试集的目标值
    print("准确率：", score)

    # 查看最佳参数：best_params_
    print("最佳参数：", estimator.best_params_)
    # 最佳结果：best_score_
    print("最佳结果：", estimator.best_score_)
    # 最佳估计器：best_estimator_
    print("最佳估计器：", estimator.best_estimator_)
    # 交叉验证结果：cv_results_
    print("交叉验证结果:", estimator.cv_results_)


if __name__ == '__main__':
    unde()

```



##### **SVM(支持向量机)**

**SVM（支持向量机）则是通过寻找`分类超平面进而最大化类别间隔`实现分类；**

**线性可分样本集：只要我们可以使用一条直线将样本集完全分开，如图.**

**![10](图\10.png)**

**非线性可分样本集：没有办法直接画一条直线，只能用曲线等模式来分开的样本集，如图**

**![11](图\11.png)**

**SVM分类器**

```

```





**![5](图\5.png)**

#### **回归算法**

##### **线性回归**

**1.线性回归应用场景**

- **房价预测**
- **销量额度预测**
- **贷款额度预测**

**2.线性回归是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式**

**特点：只有一个自变量的情况称为单变量回归，对于一个自变量情况的叫做多元回归**

**3.线性回归的损失和优化原理**

- **损失函数：最小二乘法**
- **优化算法**
  - **正规方程：直接求解w**
  - **梯度下降：试错，改进**

**4.线性回归API**

**(1)线性回归**

```py
sklearn.linear_model.LinearRegression(fit_intercept=True)
```

- **fit_intercept：是否计算偏置**
- **LinearRegression.coef_：回归系数**
- **LinearRegression.intercept_：偏置**

##### **(2)梯度下降**

```py
sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate='invscaling', eta0=0.01)
```

**![6](图\6.png)**

**回归性能评估**

```py
sklearn.metrics.mean_squared_error(y_ture, y_pred)
```

- **均方误差回归损失**
- **y_true：真实值**
- **y_pred：预测值**
- **return：浮点数结果**



**波士顿房价预估**

**流程：**
**1）获取数据集**
**2）划分数据集**
**3）特征工程：无量纲化 - 标准化**
**4）预估器流程：fit() -> 模型，coef_ intercept_**
**5）模型评估**

```py
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.metrics import mean_squared_error


def linner1():
    """
    正规方程的优化方法
    :return:
    """
    # 1）获取数据
    boston = load_boston()

    # 2）划分数据集
    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)

    # 3）标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)

    # 4）预估器
    estimator = LinearRegression()
    estimator.fit(x_train, y_train)

    # 5）得出模型
    print("正规方程权重系数为：\n", estimator.coef_)
    print("正规方程偏置为：\n", estimator.intercept_)

    # 6）模型评估
    y_predict = estimator.predict(x_test)
    print("预测房价：\n", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("正规方程-均分误差为：\n", error)

    return None


def linner2():
    """
    梯度下降的优化方法
    :return:
    """
    # 1）获取数据
    boston = load_boston()
    print("特征数量：\n", boston.data.shape)  # 几个特征对应几个权重系数

    # 2）划分数据集
    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)

    # 3）标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)

    # 4）预估器
    estimator = SGDRegressor(learning_rate="constant", eta0=0.001, max_iter=10000)
    estimator.fit(x_train, y_train)

    # 5）得出模型
    print("梯度下降权重系数为：\n", estimator.coef_)
    print("梯度下降偏置为：\n", estimator.intercept_)

    # 6）模型评估
    y_predict = estimator.predict(x_test)
    print("预测房价：\n", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("梯度下降-均分误差为：\n", error)

    return None


if __name__ == '__main__':
    linner1()
    linner2()

```

**![7](图\7.png)**

**总结**

**线性回归的损失函数：均方误差**
**线性回归的优化方法：正规方程、梯度下降**
**线性回归的性能衡量方法：均方误差**



##### **逻辑回归**

**逻辑回归API**

```py

```

- **y_true：真实目标值**
- **y_pred：估计器预测目标值**
- **labels:指定类别对应的数字**
- **target_names：目标类别名称**
- **return：每个类别精确率与召回率**

```py
"""
案例：癌症分类预测-良／恶性乳腺癌肿瘤预测
原始数据的下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases/
数据描述
（1）699条样本，共11列数据，第一列用语检索的id，后9列分别是与肿瘤相关的医学特征，最后一列表示肿瘤类型的数值。
（2）包含16个缺失值，用”?”标出。
（3）最后一列表示肿瘤类型的数值：2代表良性；4代表恶行性

实现步骤：
1.获取数据
2.基本数据处理
2.1 缺失值处理
2.2 确定特征值,目标值
2.3 分割数据
3.特征工程(标准化)
4.机器学习(逻辑回归)
5.模型评估
"""


from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# 1.获取数据
names = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',
                   'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',
                   'Normal Nucleoli', 'Mitoses', 'Class']

data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', names=names)

# 数据处理
data = data.replace(to_replace="?", value=np.NaN)
data = data.dropna()

# 数据切分
x = data.iloc[:, 1:10]
y = data["Class"]

# 分割数据
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22)

# 特征工程 标准化
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test =transfer.fit_transform(x_test)

# 逻辑回归
es = LogisticRegression()
es.fit(x_train, y_train)

# 评估模型
print(es.predict(x_test))

from sklearn.metrics import classification_report

ret = classification_report(y_test, es.predict(x_test), labels=(2, 4), target_names=("良性", "恶性"))  # 分类评估
print(ret)

```

**运行结果**

**![9](C:\Users\Administrator\Desktop\机器学习\图\9.png)**



#### **无监督学习-聚类**

##### **K-means**

**K-means原理**

**![8](C:\Users\Administrator\Desktop\机器学习\图\8.png)**

**K-means原理API**

```py
sklearn.cluster.KMeans(n_cluster=8, init='k-means++')
```

- **n_clusters：开始聚类中心数量**
- **init：初始化方法，默认为‘k-means++’**
- **labels_：默认标记的类型，可以和真实值比较（不是值比较）**













**sklearn中常用的模块有分类、回归、聚类、降维、模型选择、预处理。**

**分类：识别某个对象属于哪个类别，常用的算法有：SVM（支持向量机）、nearest neighbors（最近邻）、random forest（随机森林），常见的应用有：垃圾邮件识别、图像识别。**

**回归：预测与对象相关联的连续值属性，常见的算法有：SVR（支持向量机）、 ridge regression（岭回归）、Lasso，常见的应用有：药物反应，预测股价。**

**聚类：将相似对象自动分组，常用的算法有：k-Means、 spectral clustering、mean-shift，常见的应用有：客户细分，分组实验结果。**

**降维：减少要考虑的随机变量的数量，常见的算法有：PCA（主成分分析）、feature selection（特征选择）、non-negative matrix factorization（非负矩阵分解），常见#的应用有：可视化，提高效率。**

**模型选择：比较，验证，选择参数和模型，常用的模块有：grid search（网格搜索）、cross validation（交叉验证）、metrics（度量）。它的目标是通过参数调整提高精度。**

**预处理：特征提取和归一化，常用的模块有：preprocessing，feature extraction，常见的应用有：把输入数据（如文本）转换为机器学习算法可用的数据。**



**常用的模型和算法以及它们适用的场景：**

1. **线性回归：适用于连续值的预测问题；**
2. **逻辑回归：适用于二分类问题；**
3. **决策树：适用于分类和回归问题；**
4. **随机森林：适用于分类和回归问题，能够处理高维数据和非线性关系；**
5. **支持向量机（SVM）：适用于分类和回归问题，对数据量较小且维度较高的情况表现较好；**
6. **K近邻（KNN）：适用于分类和回归问题，对数据分布的情况不敏感。**



